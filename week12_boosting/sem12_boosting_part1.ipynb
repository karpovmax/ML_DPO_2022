{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf84c1b0",
   "metadata": {
    "id": "fitted-subscriber"
   },
   "source": [
    "## Семинар 12: Бустинг, часть 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d122977",
   "metadata": {
    "id": "canadian-design"
   },
   "source": [
    "### 1. Бустинг: идея подхода"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874c47e6",
   "metadata": {
    "id": "patient-watson"
   },
   "source": [
    "На прошлом семинаре мы рассмотрели один из способов построения композиций моделей – бэггинг. Вспомним, что основная идея бэггинга состоит в том, чтобы агрегировать ответы большого числа независимых переобученных моделей, имеющих низкое смещение и большую дисперсию. Агрегация позволяет добиться значительного снижения дисперсии при сохранении низкого смещения. Бэггинг над деревьями, в котором при построении каждого дерева при разбиениях используются случайные подмножества признаков, называется случайным лесом.\n",
    "\n",
    "На этом семинаре мы рассмотрим альтернативный бэггингу подход к построению композиции моделей – бустинг. Идея бустинга состоит в том, чтобы построить последовательность **базовых моделей**, каждая из которых **исправляет ошибки предыдущих**. Как и в случае бэггинга, ответ композиции получается агрегированием ответов базовых моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1350af5a",
   "metadata": {
    "id": "working-illustration"
   },
   "source": [
    "#### 1.1 Простой пример для понимания идеи. \n",
    "\n",
    "Пусть мы решаем задачу регрессии на тренировочной выборке $(X, y)$. Мы обучаем модель $a(X)$, которая является композицией базовых моделей $b_n(X)$. Будем считать, что композиция строится простым суммированием ответов базовых моделей:\n",
    "\n",
    "$$\n",
    "a(X) = \\sum_{n = 1}^{k} b_n(X).\n",
    "$$\n",
    "\n",
    "В качестве базовой модели будем использовать решающее дерево глубины один (decision stump). \n",
    "\n",
    "**Важное замечание.** Считается, что одним из наиболее сильных и универсальных методов машинного обучения на данный момент является бустинг над деревьями, и во многих туториалах в Интернете рассматривается именно бустинг над деревьями. Следуя этой традиции (а также потому что при использовании деревьев картинки получаются более наглядными), в этом примере мы также рассмотрим бустинг над деревьями. Однако важно понимать, что бустинг – это подход, который может строиться над любыми моделями, на что мы также обратим внимание ниже.\n",
    "\n",
    "Будем решать задачу при помощи минимизации среднеквадратичной ошибки:\n",
    "\n",
    "$$\n",
    "\\dfrac{1}{\\ell}\\sum_{i = 1}^{\\ell} (a(x_i) - y_i)^2 \\to \\min_a.\n",
    "$$\n",
    "\n",
    "В этом примере будем работать только с обучающей выборкой. Для построения композиции непосредственно используем идею бустинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c468b9",
   "metadata": {
    "id": "developed-salad"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc1e7eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "adjustable-absolute",
    "outputId": "2e746c1d-704f-473f-e27d-cc2479140803"
   },
   "outputs": [],
   "source": [
    "# Генерация выборки\n",
    "np.random.seed(123)\n",
    "N = 100\n",
    "X = np.linspace(0, 1, N).reshape(-1, 1)\n",
    "y = np.sin(X)[:, 0] + np.random.normal(0, 0.1, size = N)\n",
    "\n",
    "# Функция для визуализации выборки и предсказаний\n",
    "def plot_sample_model(X, y, plot_predictions = False, y_pred = None, y_pred_label = None, loss = 'mse'):\n",
    "    plt.figure(figsize = (10, 7))\n",
    "    plt.scatter(X, y, label = 'Обучающая выборка', alpha = 0.7)\n",
    "    if plot_predictions:\n",
    "        plt.plot(X, y_pred, label = y_pred_label, c = 'r')\n",
    "        if loss == 'mse':\n",
    "            plt.title('MSE: ' + str(mean_squared_error(y, y_pred)))\n",
    "        elif loss == 'mae':\n",
    "            plt.title('MAE: ' + str(mean_absolute_error(y, y_pred)))\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend();\n",
    "    \n",
    "plot_sample_model(X, y, plot_predictions = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae6bc3c",
   "metadata": {
    "id": "sustained-logistics"
   },
   "source": [
    "**Шаг 0.** Имеем пустую композицию решающих деревьев $a(X) = \\{\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac17c6b7",
   "metadata": {
    "id": "jewish-sharing"
   },
   "outputs": [],
   "source": [
    "a = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0479fe",
   "metadata": {
    "id": "naughty-interim"
   },
   "source": [
    "**Шаг 1.** Обучим первое решающее дерево $DT_1$ и включим его в композицию: $a^{(1)}(X) = DT_1$. Получим предсказания композиции на обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47401b71",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "coastal-thesis",
    "outputId": "90ed7349-e2fa-47b6-b8d0-78b531207e93"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# TODO: обучите решающее дерево глубины 1 и сделайте предсказания на обучающей выборке\n",
    "dt1 = DecisionTreeRegressor(max_depth = 1)\n",
    "dt1.fit(X, y)\n",
    "dt1_pred = dt1.predict(X)\n",
    "\n",
    "# TODO: включите предсказания первого дерева в композицию (простым суммированием)\n",
    "a = dt1_pred\n",
    "\n",
    "# Визуализация выборки и предсказаний\n",
    "plot_sample_model(X, y, plot_predictions = True, y_pred = a, y_pred_label = 'Предсказания a = DT1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120eaca7",
   "metadata": {
    "id": "grand-station"
   },
   "source": [
    "**Шаг 2.** В качестве ошибок композиции будем использовать **остатки** – \n",
    "расстояния от предсказаний композиции до истинных ответов:\n",
    "\n",
    "$$\n",
    "s^{(1)}(X) = y - a^{(1)}(X)\n",
    "$$\n",
    "\n",
    "Заметим, что $MSE$ – это усреднённая сумма квадратов остатков и что остатки являются естественным выражением ошибки: если мы прибавим остатки к ответам модели, то добьёмся нулевой среднеквадратичной ошибки. Таким образом, логично строить следующую модель так, чтобы она хорошо предсказывала остатки композиции: если мы прибавим предсказания второй модели (предсказанные остатки) к предскзазаниям первой, то добьёмся снижения ошибки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2874d",
   "metadata": {
    "id": "familiar-sessions"
   },
   "outputs": [],
   "source": [
    "# TODO: рассчитайте остатки  \n",
    "s1 = y - a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff32643f",
   "metadata": {
    "id": "improved-brake"
   },
   "source": [
    "**Шаг 3.** Обучим второе решающее дерево $DT_2$, предсказывающее остатки. Добавим предсказания второго дерева в композицию: $a^{(2)}(X) = DT_1 + DT_2$. Получим предсказания композиции на обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfea17e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "knowing-albert",
    "outputId": "d0bb08d6-bab8-4a89-f55c-945e3d50898f"
   },
   "outputs": [],
   "source": [
    "# TODO: обучите второе решающее дерево глубины 1, предсказывающее остатки\n",
    "dt2 = DecisionTreeRegressor(max_depth = 1)\n",
    "dt2.fit(X, s1)\n",
    "\n",
    "# TODO: сделайте предсказания на обучающей выборке\n",
    "dt2_pred = dt2.predict(X)\n",
    "\n",
    "# TODO: включите предсказания второго дерева в композицию (простым суммированием)\n",
    "a += dt2_pred\n",
    "\n",
    "# Визуализация выборки и предсказаний\n",
    "plot_sample_model(X, y, plot_predictions = True, y_pred = a, y_pred_label = 'Предсказания a = DT1 + DT2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d4c5b8",
   "metadata": {
    "id": "funded-extreme"
   },
   "source": [
    "Как мы видим, решающая поверхность стала более сложной, и теперь она более точно приближает обучающую выборку. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0513ea0b",
   "metadata": {
    "id": "appropriate-passion"
   },
   "source": [
    "**Шаги 4 – ...** Повторяем шаги 2-3, пока не надоест."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3903f5d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "saved-longer",
    "outputId": "77f4846e-b4c5-45a3-dac1-3d10a904df16"
   },
   "outputs": [],
   "source": [
    "# TODO: рассчитайте остатки\n",
    "s2 = y - a\n",
    "\n",
    "# TODO: обучите третье решающее дерево глубины 1, предсказывающее остатки\n",
    "dt3 = DecisionTreeRegressor(max_depth = 1)\n",
    "dt3.fit(X, s2)\n",
    "\n",
    "# TODO: сделайте предсказания на обучающей выборке\n",
    "dt3_pred = dt3.predict(X)\n",
    "\n",
    "# TODO: включите предсказания третьего дерева в композицию (простым суммированием)\n",
    "a += dt3_pred\n",
    "\n",
    "# Визуализация выборки и предсказаний\n",
    "plot_sample_model(X, y, plot_predictions = True, y_pred = a, y_pred_label = 'Предсказания DT1 + DT2 + DT3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b889db07",
   "metadata": {
    "id": "certified-applicant"
   },
   "source": [
    "Как мы видим, при добавлении базовых моделей решающая поверхность становится более сложной и всё точнее приближает обучающую выборку."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41bdb3d",
   "metadata": {
    "id": "integrated-northeast"
   },
   "source": [
    "**Важный момент:** заметим, что при обучении очередной базовой модели композиция предыдущего шага считается фиксированной. Это означает, что обучение новой базовой модели не влияет на уже обученные модели, содержащиеся в композиции."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861b6908",
   "metadata": {
    "id": "polished-arrival"
   },
   "source": [
    "#### 1.2 Бустинг над разными моделями."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6064d8ae",
   "metadata": {
    "id": "responsible-walter"
   },
   "source": [
    "При помощи функции `plot_boosting_results()` можно визуализировать решающие поверхности композиции, очередной базовой модели и остатки при добавлении базовых моделей. \n",
    "\n",
    "**Задание:** посмотрите, как выглядят решающие поверхности композиции, если в качестве базовой модели использовать\n",
    "- решающее дерево\n",
    "- линейную регрессию\n",
    "- `kNN`-регрессор\n",
    "- `SVR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a41a36",
   "metadata": {
    "id": "supposed-syracuse"
   },
   "outputs": [],
   "source": [
    "def plot_boosting_results(base_estimator, n_estimators, X, y):\n",
    "    \n",
    "    fig, ax = plt.subplots(n_estimators, 3, figsize = (20, n_estimators * 5))\n",
    "    \n",
    "    # Остатки\n",
    "    resid = []\n",
    "    resid.append(y)\n",
    "    \n",
    "    # Предсказания моделей\n",
    "    y_pred = []\n",
    "    \n",
    "    for i in range(n_estimators):\n",
    "        # Обучение очередной базовой модели\n",
    "        base_estimator.fit(X, resid[-1])\n",
    "        \n",
    "        # Предсказание базовой модели\n",
    "        y_pred.append(base_estimator.predict(X))\n",
    "        \n",
    "        # Вычисление предсказания композиции\n",
    "        a = np.sum(y_pred, axis = 0)\n",
    "        \n",
    "        # Вычисление остатка\n",
    "        resid.append(y - a)\n",
    "        \n",
    "        # Предсказания композиции\n",
    "        ax[i, 0].scatter(X, y, label = 'Обучающая выборка', alpha = 0.7)\n",
    "        ax[i, 0].plot(X, a, c = 'red', lw = 3, label = 'Число деревьев: ' + str(i + 1))\n",
    "        ax[i, 0].set_title('MSE: ' + str(mean_squared_error(y, a)))\n",
    "\n",
    "        # Предсказания очередной модели\n",
    "        ax[i, 1].scatter(X, y, label = 'Обучающая выборка', alpha = 0.7)\n",
    "        ax[i, 1].plot(X, y_pred[-1], c = 'red', lw = 3)\n",
    "        ax[i, 1].set_title('Предсказания модели ' + str(i + 1))\n",
    "        \n",
    "        ax[i, 2].scatter(X, resid[-1], alpha = 0.7, marker = 'v', c = 'orange')\n",
    "        ax[i, 2].set_title('Остатки')\n",
    "        \n",
    "        ax[i, 0].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808b811",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 841
    },
    "id": "later-utility",
    "outputId": "17e2bf5c-fd2e-41f1-a327-83adc49fc9cf"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "plot_boosting_results(SVR(kernel='linear'), n_estimators = 3, X = X, y = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15181ad",
   "metadata": {
    "id": "changing-communications"
   },
   "source": [
    "### 2. Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6a403",
   "metadata": {
    "id": "biological-restriction"
   },
   "source": [
    "Заметим, что в примере выше использование остатков как меры ошибки модели было оправданным, так как остатки непосредственно участвуют в расчёте $MSE$. Попробуем обобщить идею использования остатков на произвольную функцию потерь.\n",
    "\n",
    "Будем решать ту же задачу, что и выше, но с использованием произвольной функции потерь $L(\\cdot)$. Предположим, что каким-то образом мы уже обучили $N$ базовых моделей и построили композицию $a^{(N)}(X)$. Как построить $(N+1)$-ю базовую модель?\n",
    "\n",
    "Заметим, что мы хотим выбрать $(N+1)$-ую базовую модель так, чтобы как можно сильнее уменьшить ошибку:\n",
    "\n",
    "$$\n",
    "\\dfrac{1}{\\ell}\\sum_{i = 1}^{\\ell} L[y_i, a^{N}(x_i) + b^{(N+1)}(x_i)] \\to \\min_{b^{(N+1)}},\n",
    "$$\n",
    "\n",
    "где $b^{(N+1)}(x_i)$ – предсказания новой базовой модели. Теперь заметим, что так как и $y_i$, и $a^{N}(x_i)$ фиксированы (обсуждали выше), то в сущности нам нужно найти числа $b^{(N+1)}(x_i) = s_i^{(N)}$, которые сильнее всего уменьшили бы функцию потерь:\n",
    "\n",
    "\n",
    "$$\n",
    "\\dfrac{1}{\\ell}\\sum_{i = 1}^{\\ell} L[y_i, a^{N}(x_i) + s^{(N)}_i] \\to \\min_{s^{(N)}_i}.\n",
    "$$\n",
    "\n",
    "**Замечание.** Заметьте, что теперь мы ищем просто числа $s^{(N)}_i$, которые сильнее всего уменьшили бы функцию.\n",
    "\n",
    "После изучения градиентного спуска мы знаем, что вектор чисел, сильнее всего уменьшающих какую-то функцию – это антиградиент этой функции. Таким образом, в качестве чисел $s_i^{(N)}$ следует взять\n",
    "\n",
    "$$\n",
    "s_i^{(N)} = -\\dfrac{\\partial L(y, p)}{\\partial p}\n",
    "$$\n",
    "\n",
    "в точке $p = a^{(N)}(x_i)$, то есть антиградиент функции потерь в точке ответов уже построенной композиции. \n",
    "\n",
    "**Чек-пойнт.** Убедитесь, что вы понимаете, какие **числа**, нужно взять, чтобы решить задачу. \n",
    "\n",
    "Итак, мы выяснили, какие числа нужно взять для уменьшения функции потерь на **обучающей выборке**. Вспомним, что мы определяли эти числа как $b^{(N+1)}(x_i) = s_i^{(N)}$, то есть ответы очередной базовой модели. Таким образом, нам нужно обучить очередную базовую модель так, чтобы она хорошо приближала антиградиент функции потерь в точке ответов уже построенной композиции – а это хорошо известная нам задача обучения с учителем. Обычно она решается путём минимизации $MSE$:\n",
    "\n",
    "$$\n",
    "b^{(N+1)}(x) = \\arg\\min_b \\sum_{i = 1}^{\\ell} (b(x_i) - s_i^{(N)})^2.\n",
    "$$\n",
    "\n",
    "**Важный момент:** заметим, что задача, описанная выше – это не изначальная задача! В этом и состоит магия градиентного бустинга: мы можем использовать любую функцию потерь, через неё вывести $s_i^{(N)}$, а затем просто решать задачу приближения при помощи $MSE$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e60e3f",
   "metadata": {
    "id": "regional-ireland"
   },
   "source": [
    "#### 2.1 Продолжение простого примера для понимания идеи.\n",
    "\n",
    "Повторим шаги из примера 1.1, используя градиентный бустинг, чтобы лучше разобраться в идее и провести параллели. Вспомним, что мы решали задачу регрессии при помощи минимизации $MSE$.\n",
    "\n",
    "Нулевой и первый шаги в этих примерах совпадают."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772987c8",
   "metadata": {
    "id": "complex-specialist"
   },
   "source": [
    "**Шаг 0.** Имеем пустую композицию решающих деревьев $a(X) = \\{\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db2323",
   "metadata": {
    "id": "owned-freedom"
   },
   "outputs": [],
   "source": [
    "a = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4bf91",
   "metadata": {
    "id": "private-issue"
   },
   "source": [
    "**Шаг 1.** Обучим первое решающее дерево $DT_1$ и включим его в композицию: $a^{(1)}(X) = DT_1$. Получим предсказания композиции на обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cca313",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "proved-vehicle",
    "outputId": "2d468868-fce8-4a52-d06d-f6575cc219d1"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Обучение решающего дерева глубины 1 и получение предсказаний на обучающей выборке\n",
    "dt1 = DecisionTreeRegressor(max_depth = 1)\n",
    "dt1.fit(X, y)\n",
    "dt1_pred = dt1.predict(X)\n",
    "\n",
    "# Включение предсказаний первого дерева в композицию (простым суммированием)\n",
    "a = dt1_pred\n",
    "\n",
    "# Визуализация выборки и предсказаний\n",
    "plot_sample_model(X, y, plot_predictions = True, y_pred = a, y_pred_label = 'Предсказания a = DT1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d24b16",
   "metadata": {
    "id": "agreed-lover"
   },
   "source": [
    "**Шаг 2.** В примере 1.1 на этом шаге мы рассчитывали остатки. Здесь же мы должны рассчитать антиградиент функции потерь в точке ответов уже построенной композиции.\n",
    "\n",
    "**Задание:** вычислите градиент $MSE$\n",
    "\n",
    "$$\n",
    "L(y, p) = \\dfrac{1}{2} \\sum_{i = 1}^{\\ell} (y_i - p_i)^2\n",
    "$$\n",
    "\n",
    "по $p_i$ в точке $p_i = a(x_i)$.\n",
    "\n",
    "**Решение:** \n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L(y, p)}{\\partial p_i} = -(y_i - p_i) = \\{p_i = a(x_i)\\} = -(y_i - a(x_i)).\n",
    "$$\n",
    "\n",
    "Таким образом, антиградиент функции потерь равен:\n",
    "\n",
    "$$\n",
    "s_i = - (-(y_i - a(x_i))) = y_i - a(x_i),\n",
    "$$\n",
    "\n",
    "то есть в точности остаток в том виде, как мы определяли выше. \n",
    "\n",
    "Таким образом, использование градиентного бустинга, если целевая функция – $MSE$, эквивалентно процедуре расчёта остатков, как мы это делали в примере 1.1, а сами остатки совпадают с антиградиентами $s_i$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862a553",
   "metadata": {
    "id": "pressed-village"
   },
   "outputs": [],
   "source": [
    "# TODO: рассчитайте антиградиент  \n",
    "s1 = y - a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4e94c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "imposed-future",
    "outputId": "460d2ddc-56b6-4ce3-c026-800f6a72e23d"
   },
   "outputs": [],
   "source": [
    "# Обучение второго решающего дерева глубины 1, предсказывающего антиградиент\n",
    "dt2 = DecisionTreeRegressor(max_depth = 1)\n",
    "dt2.fit(X, s1)\n",
    "\n",
    "# Получение предсказаний на обучающей выборке\n",
    "dt2_pred = dt2.predict(X)\n",
    "\n",
    "# Включение предсказаний второго дерева в композицию (простым суммированием)\n",
    "a += dt2_pred\n",
    "\n",
    "# Визуализация выборки и предсказаний\n",
    "plot_sample_model(X, y, plot_predictions = True, y_pred = a, y_pred_label = 'Предсказания a = DT1 + DT2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f949dd",
   "metadata": {
    "id": "concerned-fifteen"
   },
   "source": [
    "**Шаг 3 и далее.** Далее нам нужно обучить базовую модель, предсказывающую $s_i$. Но так как $s_i$ совпадают с остатками, этот и дальнейшие шаги будут совпадать с примером 1.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ebfba",
   "metadata": {
    "id": "average-tumor"
   },
   "source": [
    "### 3. Функции потерь градиентного бустинга для регрессии и классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468fc82",
   "metadata": {
    "id": "becoming-causing"
   },
   "source": [
    "#### 3.1 Регрессия."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6bfbb3",
   "metadata": {
    "id": "referenced-feeding"
   },
   "source": [
    "В задачах регрессии в качестве целевой функции для композиции обычно берутся:\n",
    "\n",
    "1. $MSE$, которую мы рассматривали ранее.\n",
    "\n",
    "2. $MAE$:\n",
    "\n",
    "$$\n",
    "MAE(y, p) = \\dfrac{1}{\\ell} \\sum_{i = 1}^{\\ell} |y_i - p_i|\n",
    "$$\n",
    "\n",
    "Попробуем сделать шаг градиентного бустинга, используя в качестве функции потерь $MAE$.\n",
    "\n",
    "**Задание:** рассчитайте $s_i$ для $MAE$.\n",
    "\n",
    "**Решение:** $s_i^{(N)} = -\\mathrm{sign}(a^{(N-1)}(x_i) - y_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc795fbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "comic-above",
    "outputId": "a951b528-2caa-4667-80e5-b55c5cdf8c5b"
   },
   "outputs": [],
   "source": [
    "# Инициализация\n",
    "a = 0\n",
    "\n",
    "# Обучение решающего дерева глубины 1 и получение предсказаний на обучающей выборке\n",
    "dt1 = DecisionTreeRegressor(max_depth = 1)\n",
    "dt1.fit(X, y)\n",
    "dt1_pred = dt1.predict(X)\n",
    "\n",
    "# Включение предсказаний первого дерева в композицию (простым суммированием)\n",
    "a = dt1_pred\n",
    "\n",
    "# Визуализация выборки и предсказаний\n",
    "plot_sample_model(X, y, plot_predictions = True, y_pred = a, y_pred_label = 'Предсказания a = DT1', loss = 'mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5be1fc",
   "metadata": {
    "id": "breeding-warehouse"
   },
   "outputs": [],
   "source": [
    "# TODO: рассчитайте антиградиент  \n",
    "s1 = -np.sign(a - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70cb93c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "reliable-actor",
    "outputId": "7e102329-b461-4594-cb1c-693838c90795"
   },
   "outputs": [],
   "source": [
    "# Обучение второго решающего дерева глубины 1, предсказывающего антиградиент\n",
    "dt2 = DecisionTreeRegressor(max_depth = 1)\n",
    "dt2.fit(X, s1)\n",
    "\n",
    "# Получение предсказаний на обучающей выборке\n",
    "dt2_pred = dt2.predict(X)\n",
    "\n",
    "# Включение предсказаний второго дерева в композицию (простым суммированием)\n",
    "a += dt2_pred\n",
    "\n",
    "# Визуализация выборки и предсказаний\n",
    "plot_sample_model(X, y, plot_predictions = True, y_pred = a, \n",
    "                  y_pred_label = 'Предсказания a = DT1 + DT2', loss = 'mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71771d71",
   "metadata": {
    "id": "needed-farmer"
   },
   "source": [
    "Заметим, что $MAE$ увеличилась, а решающая поверхность имеет \"виток\", уходящий за пределы обучающей выборки. Эту проблему мы решим позже."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd92f6",
   "metadata": {
    "id": "acting-prerequisite"
   },
   "source": [
    "#### 3.2 Классификация."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce93eae7",
   "metadata": {
    "id": "explicit-breakdown"
   },
   "source": [
    "В задачах бинарной классификации обычно используется логистическая функция потерь, с которой мы уже сталкивались в логистической регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dd4a6b",
   "metadata": {
    "id": "modular-southwest"
   },
   "source": [
    "$$\n",
    "L(y, p) = \\log(1 + e^{-yp}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d0b01",
   "metadata": {
    "id": "allied-consistency"
   },
   "source": [
    "**Бонусное задание:** покажите, что в этом случае антиградиенты можно вычислить по формуле\n",
    "\n",
    "$$\n",
    "s^{(N)}_i = \\dfrac{y_i}{1 + e^{y_ia^{(N-1)}(x_i)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c7f90",
   "metadata": {
    "id": "adult-operation"
   },
   "source": [
    "Попробуем сделать шаг градиентного бустинга в задаче бинарной классификации с логистической функцией потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da46f29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "public-promise",
    "outputId": "81bd6772-cae4-4221-9f60-27a5fef73c07"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Загрузка данных\n",
    "data = load_iris()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# Перекодируем в задачу бинарной классификации\n",
    "y[y == 2] = 1\n",
    "set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30187456",
   "metadata": {
    "id": "standard-compensation"
   },
   "outputs": [],
   "source": [
    "# Инициализация\n",
    "a = 0\n",
    "\n",
    "# Обучение решающего дерева глубины 1 и получение предсказаний на обучающей выборке\n",
    "dt1 = DecisionTreeRegressor(max_depth = 1)\n",
    "dt1.fit(X, y)\n",
    "dt1_pred = dt1.predict(X)\n",
    "\n",
    "# Включение предсказаний первого дерева в композицию (простым суммированием)\n",
    "a = dt1_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222380c8",
   "metadata": {
    "id": "unusual-turtle"
   },
   "outputs": [],
   "source": [
    "def log_loss(y, y_pred):\n",
    "    # TODO: реализуйте логистическую функцию потерь\n",
    "    return np.log(1 + np.exp(- y * y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918253d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "interesting-picture",
    "outputId": "5e5f0d8e-0598-45d3-a8f1-b6e710815d5a"
   },
   "outputs": [],
   "source": [
    "# Вычисление ошибки для одного дерева\n",
    "log_loss(y, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c2683c",
   "metadata": {
    "id": "acting-concentration"
   },
   "outputs": [],
   "source": [
    "# TODO: рассчитайте антиградиент  \n",
    "s1 = y / (1 + np.exp(y * a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ffdefd",
   "metadata": {
    "id": "announced-allocation"
   },
   "outputs": [],
   "source": [
    "# Обучение второго решающего дерева глубины 1, предсказывающего антиградиент\n",
    "dt2 = DecisionTreeRegressor(max_depth = 1)\n",
    "dt2.fit(X, s1)\n",
    "\n",
    "# Получение предсказаний на обучающей выборке\n",
    "dt2_pred = dt2.predict(X)\n",
    "\n",
    "# Включение предсказаний второго дерева в композицию (простым суммированием)\n",
    "a += dt2_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ae113",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "isolated-staff",
    "outputId": "35cf121c-4ec8-49bf-b3ca-61ba44ab8c43"
   },
   "outputs": [],
   "source": [
    "# Вычисление ошибки для двух деревьев\n",
    "log_loss(y, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91105373",
   "metadata": {
    "id": "undefined-cathedral"
   },
   "source": [
    "### 4. Особенности градиентного бустинга"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77aca14",
   "metadata": {
    "id": "innocent-bride"
   },
   "source": [
    "#### 4.1 Построение композиции."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b749065",
   "metadata": {
    "id": "united-commission"
   },
   "source": [
    "В примерах выше мы строили композицию моделей простым суммированием. Понятно, что в общем виде можно строить взвешенную сумму базовых моделей:\n",
    "\n",
    "$$\n",
    "a(X) = \\sum_{n = 1}^{k} w_nb_n(X),\n",
    "$$\n",
    "\n",
    "где коэффициенты $w_n$ можно подобрать, например, при помощи градиентного спуска в задаче \n",
    "\n",
    "$$\n",
    "w_n = \\arg\\min_{w} \\sum_{i = 1}^{\\ell} L(y_i, a^{(N-1)}(x_i) + w \\times b_N(x_i)),\n",
    "$$\n",
    "\n",
    "что имеет смысл, так как $b_N(x_i)$ уже обучена и фиксирована.\n",
    "\n",
    "**Замечание:** длина шага является одним из ключевых параметров градиентного бустинга, и может очень сильно повлиять на его производительность. Подробнее об этом [здесь](https://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/).\n",
    "\n",
    "В нашем случае подбор правильного коэффициента поможет решить увеличение MAE в примере 3.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096867c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "coordinated-freeze",
    "outputId": "dc88728c-a001-4be9-f35e-b569edf08356"
   },
   "outputs": [],
   "source": [
    "# Инициализация\n",
    "np.random.seed(123)\n",
    "N = 100\n",
    "X = np.linspace(0, 1, N).reshape(-1, 1)\n",
    "y = np.sin(X)[:, 0] + np.random.normal(0, 0.1, size = N)\n",
    "a = 0\n",
    "\n",
    "# Обучение решающего дерева глубины 1 и получение предсказаний на обучающей выборке\n",
    "dt1 = DecisionTreeRegressor(max_depth = 1)\n",
    "dt1.fit(X, y)\n",
    "dt1_pred = dt1.predict(X)\n",
    "\n",
    "# Включение предсказаний первого дерева в композицию (простым суммированием)\n",
    "a = dt1_pred\n",
    "\n",
    "# Визуализация выборки и предсказаний\n",
    "plot_sample_model(X, y, plot_predictions = True, y_pred = a, y_pred_label = 'Предсказания a = DT1', loss = 'mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db979bb4",
   "metadata": {
    "id": "incorporate-stockholm"
   },
   "outputs": [],
   "source": [
    "# Вычисление антиградиента \n",
    "s1 = -np.sign(a - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4acd836",
   "metadata": {
    "id": "controlled-cameroon"
   },
   "outputs": [],
   "source": [
    "# TODO: подберите (угадайте) вес так, чтобы уменьшить MAE\n",
    "w = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0cc93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "premier-tunisia",
    "outputId": "b2214c51-d459-47ed-d3f3-78963869ca69"
   },
   "outputs": [],
   "source": [
    "# Обучение второго решающего дерева глубины 1, предсказывающего антиградиент\n",
    "dt2 = DecisionTreeRegressor(max_depth = 1)\n",
    "dt2.fit(X, s1)\n",
    "\n",
    "# Получение предсказаний на обучающей выборке\n",
    "dt2_pred = dt2.predict(X)\n",
    "\n",
    "# Включение предсказаний второго дерева в композицию (простым суммированием)\n",
    "a += w * dt2_pred\n",
    "\n",
    "# Визуализация выборки и предсказаний\n",
    "plot_sample_model(X, y, plot_predictions = True, y_pred = a, \n",
    "                  y_pred_label = 'Предсказания a = DT1 + DT2', loss = 'mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4c4fa",
   "metadata": {
    "id": "personal-winner"
   },
   "source": [
    "#### 4.2 Переобучение."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea365f",
   "metadata": {
    "id": "incorrect-clock"
   },
   "source": [
    "Вспомним, что в случае бэггинга увеличение числа базовых моделей не приводило к переобучению. Например, в случайном лесе ошибки на тренировочной и тестовой выборках выходили на плато, начиная с какого-то числа деревьев. Понятно, что бустинг не будет обладать этим свойством *по построению*: при добавлении новых базовых моделей композиция всё точнее будет приближать обучающую выборку, что в конечном итоге (зачастую быстро) может привести к переобучению. Из этого можно сделать вывод, что в случае бустинга ошибка на обучающей выборке в зависимости от числа деревьев является убывающей функцией, а ошибка на тестовой выборке, скорее всего, имеет U-образный вид. Убедимся в этом и сравним поведение ошибок и решающих поверхностей бустинга и случайного леса. \n",
    "\n",
    "Здесь и далее будем использовать [реализацию бустинга](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) из `sklearn`, которая представляет собой градиентный бустинг над деревьями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd05af9",
   "metadata": {
    "id": "special-entity"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97cb31e",
   "metadata": {
    "id": "tribal-grammar"
   },
   "outputs": [],
   "source": [
    "# Генерация выборки\n",
    "np.random.seed(123)\n",
    "X = np.linspace(0, 1, 300).reshape(-1, 1)\n",
    "\n",
    "def target(a):\n",
    "    return a > 0.5\n",
    "\n",
    "y = target(X) + np.random.normal(size = X.shape) * 0.1\n",
    "y = y[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9bfa56",
   "metadata": {
    "id": "cordless-israeli"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca672021",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "linear-necklace",
    "outputId": "60409f33-3a60-4044-d7f0-e52afe5d7c05"
   },
   "outputs": [],
   "source": [
    "# Визуализация выборки\n",
    "plt.figure(figsize = (10, 7))\n",
    "plt.scatter(X_train, y_train, label = 'Обучающая выборка')\n",
    "plt.scatter(X_test, y_test, label = 'Тестовая выборка')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4c58b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "duplicate-patient",
    "outputId": "b6381a5f-747b-43d3-e767-9a196bbc6dbe"
   },
   "outputs": [],
   "source": [
    "# Визуализация решающих поверхностей\n",
    "trees = [1, 2, 5, 20, 100, 500, 1000]\n",
    "\n",
    "fig, ax = plt.subplots(len(trees), 2, figsize = (30, 40))\n",
    "\n",
    "loss_rf_train = []\n",
    "loss_gb_train = []\n",
    "loss_rf_test = []\n",
    "loss_gb_test = []\n",
    "\n",
    "for i, ts in enumerate(trees):\n",
    "    rf = RandomForestRegressor(n_estimators = ts, max_depth = 2)\n",
    "    gb = GradientBoostingRegressor(n_estimators = ts, max_depth = 1, learning_rate = 0.1)\n",
    "    \n",
    "    rf.fit(X_train, y_train)\n",
    "    ax[i, 0].scatter(X_train, y_train, label = 'Обучающая выборка')\n",
    "    ax[i, 0].scatter(X_test, y_test, label = 'Тестовая выборка')\n",
    "    ax[i, 0].plot(sorted(X_test), rf.predict(sorted(X_test)), lw = 3, c = 'red', label = 'Предсказания на тестовой выборке')\n",
    "    ax[i, 0].set_xlabel('X')\n",
    "    ax[i, 0].set_ylabel('Y')\n",
    "    ax[i, 0].set_title('Случайный лес, число деревьев = ' + str(ts) + ', MSE = ' + str(mean_squared_error(y_test, rf.predict(X_test))))\n",
    "    ax[i, 0].legend();\n",
    "    \n",
    "    loss_rf_train.append(mean_squared_error(y_train, rf.predict(X_train)))\n",
    "    loss_rf_test.append(mean_squared_error(y_test, rf.predict(X_test)))\n",
    "    \n",
    "    gb.fit(X_train, y_train)\n",
    "    ax[i, 1].scatter(X_train, y_train, label = 'Обучающая выборка')\n",
    "    ax[i, 1].scatter(X_test, y_test, label = 'Тестовая выборка')\n",
    "    ax[i, 1].plot(sorted(X_test), gb.predict(sorted(X_test)), lw = 3, c = 'red', label = 'Предсказания на тестовой выборке')\n",
    "    ax[i, 1].set_xlabel('X')\n",
    "    ax[i, 1].set_ylabel('Y')\n",
    "    ax[i, 1].set_title('Градиентный бустинг, число деревьев = ' + str(ts) + ', MSE = ' + str(mean_squared_error(y_test, gb.predict(X_test))))\n",
    "    ax[i, 1].legend();\n",
    "    \n",
    "    loss_gb_train.append(mean_squared_error(y_train, gb.predict(X_train)))\n",
    "    loss_gb_test.append(mean_squared_error(y_test, gb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e6277",
   "metadata": {
    "id": "isolated-czech"
   },
   "source": [
    "Как мы видим, решающая поверхность случайного леса не изменяется при достижении определённого числа деревьев, в то время как для бустинга она становится всё более сложной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93915efa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "cleared-accused",
    "outputId": "80810fcc-967c-4938-871e-8d50d20b4356"
   },
   "outputs": [],
   "source": [
    "# Визуализация ошибок\n",
    "fig, ax = plt.subplots(1, 2, figsize = (20, 7))\n",
    "\n",
    "ax[0].plot(trees, loss_rf_train, label = 'MSE_Train')\n",
    "ax[0].plot(trees, loss_rf_test, label = 'MSE_Test')\n",
    "ax[0].set_xlabel('Число деревьев')\n",
    "ax[0].set_ylabel('MSE')\n",
    "ax[0].set_title('Случайный лес')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(trees, loss_gb_train, label = 'MSE_Train')\n",
    "ax[1].plot(trees, loss_gb_test, label = 'MSE_Test')\n",
    "ax[1].set_xlabel('Число деревьев')\n",
    "ax[1].set_ylabel('MSE')\n",
    "ax[1].set_title('Градиентный бустинг')\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce6e1f",
   "metadata": {
    "id": "handy-standard"
   },
   "source": [
    "Поведение ошибок для случайного леса совпадает с тем, которое мы видели на предыдущем семинаре. В случае бустинга ошибка на тренировочной выборке стабильно убывает, а на тестовой – возрастает, что вероятно говорит об (очень быстром) переобучении. Если же мы аккуратно подберём гиперпараметры для деревьев и длину шага, то сможем добиться типичной U-образной формы функции потерь на тестовой выборке.\n",
    "\n",
    "В качестве способа регуляризации также может использоваться сокращение шага:\n",
    "\n",
    "$$\n",
    "a^{(N)}(X) = a^{(N-1)}(X) + \\alpha w_N b^{(N)}(X),\n",
    "$$\n",
    "\n",
    "где $\\alpha \\in (0, 1]$ – темп обучения, или стохастический градиентный бустинг.\n",
    "\n",
    "**Важный момент:** примеры выше демонстрируют, насколько быстро бустинг может переобучаться, и как в его случае важен подбор гиперпараметров. В связи с этим может сложиться ситуация, что градиентный бустинг с дефолтными параметрами показывает более плохое качество, чем, например, случайный лес с дефолтными параметрами, однако хорошо настроенный градиентный бустинг [обычно превосходит](https://www.quora.com/How-can-the-performance-of-a-Gradient-Boosting-Machine-be-worse-than-Random-Forests) случайный лес по качеству. Важна и конкретная реализация бустинга: например, [здесь](https://towardsdatascience.com/boosting-showdown-scikit-learn-vs-xgboost-vs-lightgbm-vs-catboost-in-sentiment-classification-f7c7f46fd956), представлено очень хорошее сравнение различных имплементаций. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad015f52",
   "metadata": {
    "id": "proper-theory"
   },
   "source": [
    "#### 4.3 Пара слов о смещении и дисперсии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9cba36",
   "metadata": {
    "id": "referenced-insulin"
   },
   "source": [
    "Вспомним, что в случайном лесе в качестве базовых моделей мы брали переобученные деревья, мотивируя это тем, что бэггинг позволит сохранить низкое смещение и при этом уменьшить дисперсию. Бустинг по построению работает проивоположно: композиция обладает более низким, чем базовые модели, смещением, но такой же или большей дисперсией (ещё раз проговорите последовательность построения бустинга, и вы увидите, почему это так). Поэтому в качестве базовых моделей для бустинга обычно используются модели с высоким смещением и низкой дисперсией – как мы знаем, такие модели являются недообученными – например, неглубокие решающие деревья."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5caddf",
   "metadata": {
    "id": "floral-douglas"
   },
   "source": [
    "### 5. Тестирование градиентного бустинга на разных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6790bea2",
   "metadata": {
    "id": "smoking-valley"
   },
   "source": [
    "На прошлом семинаре мы выяснили, что случайный лес с дефолтными гиперпараметрами превосходит другие изученные нами алгоритмы на разных типах данных. Попробуем на тех же данных сравнить случайный лес и градиентный бустинг. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88ba12",
   "metadata": {
    "id": "elementary-fields"
   },
   "source": [
    "**Важное замечание:** скорее всего, бустинг с дефолтными гиперпараметрами будет показывать более плохое качество, чем случайный лес с дефолтными гиперпараметрами – ключевую роль в бустинге играет подбор гиперпараметров. В дополнение, сравнение различных имплементаций по ссылке выше показывает, что реализация `sklearn` может оказаться не самой удачной по качеству (спорный вопрос!). Поэтому попробуйте провести как можно больше экспериментов и подобрать такие гиперпараметры для бустинга, чтобы превзойти случайный лес по качеству.\n",
    "\n",
    "**Внимание:** в этой части вам предстоит скачивать объёмные наборы данных. Не забудьте удалить их после семинара, если не планируете использовать их в дальнейшем, чтобы они не занимали лишнее место на вашем компьютере.\n",
    "\n",
    "**! Модели могут обучаться очень-очень долго.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e106b6",
   "metadata": {
    "id": "distinct-mason"
   },
   "source": [
    "#### 5.1 Бинарная классификация на примере [Kaggle Predicting a Biological Response](https://www.kaggle.com/c/bioresponse/data?select=train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf6b2e",
   "metadata": {
    "id": "completed-railway"
   },
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "!wget  -O 'kaggle_response.csv' -q 'https://www.dropbox.com/s/uha70sej5ugcrur/_train_sem09.csv?dl=1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78a0d11",
   "metadata": {
    "id": "chicken-works"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('kaggle_response.csv')\n",
    "X = data.iloc[:, 1:].values\n",
    "y = data.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea196c",
   "metadata": {
    "id": "exterior-knitting"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# TODO: обучите градиентный бустинг и случайный лес с дефолтными параметрами\n",
    "# Сравните их AUC ROC на тестовой выборке\n",
    "# gb = GradientBoostingClassifier()\n",
    "# gb.fit(X_train, y_train)\n",
    "# pred_gb = gb.predict_proba(X_test)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "pred_rf = rf.predict_proba(X_test)\n",
    "\n",
    "# TODO: попробуйте настроить градиентный бустинг так, чтобы превзойти случайный лес по качеству\n",
    "gb = GradientBoostingClassifier(n_estimators = 500, learning_rate = 0.1, max_depth = 10, loss = 'exponential')\n",
    "gb.fit(X_train, y_train)\n",
    "pred_gb = gb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d93764",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "coordinated-police",
    "outputId": "d52e0432-1659-46f9-cca9-d57b16707a7f"
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, pred_gb[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d185584",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "continuing-liquid",
    "outputId": "4226a792-ae25-4f4b-edbe-711cd1641c91"
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, pred_rf[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ee2c7",
   "metadata": {
    "id": "dimensional-resolution"
   },
   "source": [
    "#### 5.2 Изображения на примере [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed841ed4",
   "metadata": {
    "id": "major-settle"
   },
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "import torchvision\n",
    "\n",
    "fmnist = torchvision.datasets.FashionMNIST('./', download = True)\n",
    "X = fmnist.data.numpy().reshape(-1, 28 * 28)\n",
    "y = fmnist.targets.numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1621d399",
   "metadata": {
    "id": "enormous-republic"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# TODO: обучите градиентный бустинг и случайный лес с дефолтными параметрами\n",
    "# Сравните их доли правильных ответов на тестовой выборке\n",
    "gb = GradientBoostingClassifier(max_depth = 1, n_estimators = 100)\n",
    "gb.fit(X_train, y_train)\n",
    "pred_gb = gb.predict(X_test)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 100, max_depth = 10)\n",
    "rf.fit(X_train, y_train)\n",
    "pred_rf = rf.predict(X_test)\n",
    "\n",
    "# TODO: попробуйте настроить градиентный бустинг так, чтобы превзойти случайный лес по качеству\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0984317",
   "metadata": {
    "id": "mighty-charge",
    "outputId": "f6e1952f-cc38-434e-c55a-9eb69da7898c"
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fefcf64",
   "metadata": {
    "id": "fifteen-office",
    "outputId": "3f6c7739-fdd5-41da-8067-deaba1084966"
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a39697",
   "metadata": {
    "id": "daily-kernel"
   },
   "source": [
    "#### 5.3 Тексты на примере бинарной классификации твитов из семинара 10\n",
    "\n",
    "Скачиваем куски датасета ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f2d195",
   "metadata": {
    "id": "broken-omega"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# Предобработка из семинара 10\n",
    "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = 'positive'\n",
    "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = 'negative'\n",
    "df = positive.append(negative)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.label, random_state=13)\n",
    "\n",
    "vec = CountVectorizer(ngram_range=(1, 1))\n",
    "bow = vec.fit_transform(X_train)\n",
    "bow_test = vec.transform(X_test)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "bow = scaler.fit_transform(bow)\n",
    "bow_test = scaler.transform(bow_test)\n",
    "\n",
    "X_train = bow\n",
    "X_test = bow_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff5d001",
   "metadata": {
    "id": "collected-pendant"
   },
   "outputs": [],
   "source": [
    "# TODO: обучите градиентный бустинг с дефолтными параметрами\n",
    "# и случайный лес с числом деревьев 100 и макс. глубиной дерева 20 \n",
    "# Сравните их доли правильных ответов на тестовой выборке\n",
    "\n",
    "gb = GradientBoostingClassifier(max_depth = 1, n_estimators = 20)\n",
    "gb.fit(X_train, y_train)\n",
    "pred_gb = gb.predict(X_test)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 20, max_depth = 20)\n",
    "rf.fit(X_train, y_train)\n",
    "pred_rf = rf.predict(X_test)\n",
    "\n",
    "# TODO: попробуйте настроить градиентный бустинг так, чтобы превзойти случайный лес по качеству\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19624854",
   "metadata": {
    "id": "concerned-circuit",
    "outputId": "cd6d395a-6550-447a-d236-1083dee9e119"
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e20540",
   "metadata": {
    "id": "sacred-safety",
    "outputId": "46abe24b-400c-4e4e-e554-4876e3eb2fcf"
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a81c03",
   "metadata": {
    "id": "certified-scottish"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "sem13_boosting_part1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
